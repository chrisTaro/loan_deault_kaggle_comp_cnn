{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDPCompChrisTaro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3W52dz9-ALu"
      },
      "source": [
        "Loan Default Prediction Competition submission\n",
        "Christian Magpantay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhadp-rF-LcL"
      },
      "source": [
        "### Install files needed and mRMR libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J0mFGf9ilS4"
      },
      "source": [
        "# https://www.tensorflow.org/neural_structured_learning\n",
        "# https://www.machinecurve.com/index.php/2021/01/07/build-an-lstm-model-with-tensorflow-and-keras/\n",
        "# https://www.tensorflow.org/tutorials/images/cnn\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw_B2VOEjQRk"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!kaggle competitions download -c loan-default-prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faSdhg_qjyvh"
      },
      "source": [
        "!unzip loan-default-prediction.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRejeI_OkXPI"
      },
      "source": [
        "!unzip test_v2.csv.zip\n",
        "!unzip train_v2.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f26Gd6YiMTOS"
      },
      "source": [
        "# install for mRMR\n",
        "!pip install numpy Cython\n",
        "!pip install -U pymrmr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bDVl5kx-SOx"
      },
      "source": [
        "### Read in csv files into pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNLM7lBm7iX_"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "tes_d = pd.read_csv('test_v2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8tZlKl7mdj"
      },
      "source": [
        "# read data\n",
        "tra_d = pd.read_csv('train_v2.csv')\n",
        "print(tra_d.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkmaPC0i-XG1"
      },
      "source": [
        "### Data cleaning by removing categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvU4Dz4N7rAa"
      },
      "source": [
        "# replace all na's and nans with zeros\n",
        "tra_d = tra_d.replace(['NA'], 0)\n",
        "tra_d = tra_d.replace([np.nan], 0)\n",
        "nonNumFeats = []\n",
        "tra_d.dropna()\n",
        "\n",
        "# remove all cols that cannot use mean/std function\n",
        "# by adding them into a list to remove\n",
        "for col in tra_d.columns[1:]:\n",
        "    try :\n",
        "        tra_d[col].mean()\n",
        "        tra_d[col].std()\n",
        "    except TypeError as e:\n",
        "        nonNumFeats.append(col)\n",
        "    except ValueError as e:\n",
        "        nonNumFeats.append(col)\n",
        "\n",
        "# drop all categorical cols from nonNumFeats list\n",
        "tra_d.drop(nonNumFeats[0:], axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsW4REyI8vm1"
      },
      "source": [
        "print(tra_d.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA7oP9lb-dlS"
      },
      "source": [
        "### Remove excessive results/loss with zeroes but keeping several hundred to prep data for discretizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nHy1Ve7tQA"
      },
      "source": [
        "# create lists with loss/result is not zero\n",
        "# and loss is equal to zero\n",
        "nonZeroesLoss = []\n",
        "zeroLoss = []\n",
        "\n",
        "# inspect loss col to find non zeroes\n",
        "for i in range(len(tra_d)):\n",
        "    if tra_d.at[i, 'loss'] != 0:\n",
        "        nonZeroesLoss.append(i)\n",
        "\n",
        "# sum the amount of non zeroes and 10000\n",
        "# to include zeroes into the dataset for mRMR\n",
        "limit = 10000 - len(nonZeroesLoss)\n",
        "x = 0\n",
        "for i in range(len(tra_d)):\n",
        "    if x == limit:\n",
        "        break\n",
        "    if tra_d.at[i, 'loss'] == 0:\n",
        "        zeroLoss.append(i)\n",
        "        x += 1\n",
        "    \n",
        "# create a list that has the rows selected for mRMR\n",
        "# and sort it in ascending order\n",
        "selec_rows = nonZeroesLoss+zeroLoss\n",
        "selec_rows.sort(reverse=False)\n",
        "\n",
        "# reload the training dataset into a new training dataset\n",
        "tra_d = pd.DataFrame(tra_d, index=selec_rows[0:])\n",
        "print(tra_d.shape)\n",
        "\n",
        "# create a copy for discretizing\n",
        "tra_discretized = tra_d.copy(deep=True)\n",
        "print(tra_discretized.head())\n",
        "\n",
        "# reduce dataset from 100,000+ -> 10,000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5gu71si7vtI",
        "outputId": "e195c2df-7615-446c-e58b-09d24bf04686"
      },
      "source": [
        "# create boundaries for each col using the mean and std of each col\n",
        "# create a left edge for numbers below mean minus the std of the col\n",
        "# create a right edge for numbers below mean and the std of the col\n",
        "# compare each cell in their respective cols to decide if:\n",
        "#   the cell's number is below the left edge = -2\n",
        "#   the cell's number is above the right edge = 2\n",
        "#   the cell's number is between the edges = 0\n",
        "for col in tra_discretized.columns[1:-1]:\n",
        "    col_mu = tra_discretized[col].mean()\n",
        "    col_std = tra_discretized[col].std()\n",
        "    left_edge = col_mu - col_std\n",
        "    right_edge = col_mu + col_std\n",
        "    for row in selec_rows:\n",
        "        val = tra_discretized.at[row, col]\n",
        "        if val < left_edge:\n",
        "            tra_discretized.at[row, col] = -2\n",
        "        elif val >= left_edge and val <= right_edge:\n",
        "            tra_discretized.at[row, col] = 0\n",
        "        elif val > right_edge:\n",
        "            tra_discretized.at[row, col] = 2\n",
        "\n",
        "print(tra_discretized.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  f1  f2   f3  f4  f5  f6  ...  f773  f774  f775  f776  f777  f778  loss\n",
            "0   1   0   2  0.0  -2   0   0  ...   0.0   0.0   0.0     2     0     0     0\n",
            "1   2   0   2  0.0  -2   0   2  ...   0.0   0.0   0.0     2     0     0     0\n",
            "2   3   0   2  0.0  -2   0   2  ...   0.0   0.0   0.0     2     0     0     0\n",
            "3   4   0   2  0.0  -2   0   2  ...   0.0   0.0   0.0     2     0     0     0\n",
            "4   5  -2   0  0.0   0   0   0  ...   0.0   0.0   0.0     0     0     0     0\n",
            "\n",
            "[5 rows x 752 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZbb-uoxIQdU",
        "outputId": "5758cc5b-ecf3-4ad3-bbc3-22e0cfe4d818"
      },
      "source": [
        "print(tra_discretized.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 752)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjZTYDbK-k7j"
      },
      "source": [
        "### Run MRMR to find and remove redundant columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA_UGH_87xhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa721dd2-84d6-45d0-a856-aa74dfa3bc9b"
      },
      "source": [
        "# create a list from the returned list of pymrmr\n",
        "# use list to remove cols\n",
        "import pymrmr\n",
        "selec_feats = pymrmr.mRMR(tra_discretized[tra_discretized.columns[1:-1]], 'MID', 23)\n",
        "tra_d.drop(selec_feats[0:], axis = 1, inplace = True)\n",
        "print(tra_d.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 729)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7UDVbx8-zMF"
      },
      "source": [
        "Prep data for testing using discretize table into CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQG2b4TMiR9X"
      },
      "source": [
        "selected_labels = pd.DataFrame(tra_d, index=selec_rows[0:], columns=['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcNaDK3XOMVb"
      },
      "source": [
        "tra_discretized.drop(selec_feats[0:], axis = 1, inplace = True)\n",
        "select_feats_df = tra_d.copy(deep=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr0Kw_NN7xbp"
      },
      "source": [
        "#  Prepare data.\n",
        "X_train = np.array(tra_discretized)\n",
        "y_train = selected_labels\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkOamh-D-4LZ"
      },
      "source": [
        "### For visuals with tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBjBTDDm-uft"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-hVjYNhhFgj"
      },
      "source": [
        "y_train = y_train.astype(float)\n",
        "some_precision = 2\n",
        "for i in selec_rows[0:]:\n",
        "    a = y_train.at[i, 'loss'] / 100\n",
        "    num = '{0:.{1}f}'.format(a,some_precision)\n",
        "    y_train.at[i, 'loss'] = num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFbSwZy-7MG"
      },
      "source": [
        "### CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrZiNcjJ70s-"
      },
      "source": [
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_step(model, optimizer, x_train, y_train):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(x_train, training=True)\n",
        "    loss = loss_object(y_train, predictions)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(y_train, predictions)\n",
        "\n",
        "X_train = X_train.reshape(len(X_train),27,27,1)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.Conv2D(16, (1,1), activation='relu', input_shape=(27,27,1)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(16, (1,1), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Conv2D(32, (1,1), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(32, (1,1), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Conv2D(64, (1,1), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (1,1), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(101, activation=\"softmax\")) # labels\n",
        "\n",
        "model.summary()\n",
        "\n",
        "op = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer='sgd',\n",
        "              metrics='accuracy')\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=1, \n",
        "                    batch_size=16, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUknc4AzraiE"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50MhHR8U--SN"
      },
      "source": [
        "### Prep for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJE7ulNwMmVX"
      },
      "source": [
        "print(tes_d.shape)\n",
        "tes_d['loss'] = 0\n",
        "X_test = np.array(tes_d)\n",
        "X_test = X_test.reshape(len(X_test),27,27,1)\n",
        "y_model = model.predict(X_test)\n",
        "print(y_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMlT8QqePLr1"
      },
      "source": [
        "print(y_model.shape)\n",
        "y_pred = [0] * len(y_model)\n",
        "for i in range(len(y_model)):\n",
        "    for j in range(len(y_model[0])):\n",
        "        if y_model[i][j] == 1.0:\n",
        "            y_pred[i] = j-1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zamVmeufSVJl"
      },
      "source": [
        "print(len(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB4YDCSQ_Aof"
      },
      "source": [
        "### Predictions for testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8xx4rT_74JK"
      },
      "source": [
        "res_df = pd.DataFrame(tes_d, columns=['id','loss'])\n",
        "for i in range(len(res_df)-3):\n",
        "    res_df['loss'].iat[i] = y_pred[i]\n",
        "print(res_df)\n",
        "sample_submission = res_df[['id','loss']]\n",
        "sample_submission.to_csv('sample_submission_rf.csv', index = False)\n",
        "# res_df['loss'].iat[-1] = y_pred[-1]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}